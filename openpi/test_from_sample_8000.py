#!/usr/bin/env python3
"""
ç›´æ¥ä»ç¬¬8000ä¸ªæ ·æœ¬å¼€å§‹æµ‹è¯•ï¼ŒéªŒè¯ä¿®å¤æ•ˆæœ
"""

import numpy as np
import tqdm
import tyro

import openpi.models.model as _model
import openpi.shared.normalize as normalize
import openpi.training.config as _config
import openpi.training.data_loader as _data_loader
import openpi.transforms as transforms


class RemoveStrings(transforms.DataTransformFn):
    def __call__(self, x: dict) -> dict:
        return {k: v for k, v in x.items() if not np.issubdtype(np.asarray(v).dtype, np.str_)}


def create_torch_dataloader(
    data_config: _config.DataConfig,
    action_horizon: int,
    batch_size: int,
    model_config: _model.BaseModelConfig,
    num_workers: int,
    max_frames: int | None = None,
    start_from_sample: int = 0,  # æ–°å¢å‚æ•°ï¼šä»å“ªä¸ªæ ·æœ¬å¼€å§‹
) -> tuple[_data_loader.Dataset, int]:
    if data_config.repo_id is None:
        raise ValueError("Data config must have a repo_id")
    if data_config.repo_id == "fake":
        return _data_loader.FakeDataset(model_config, num_samples=1024), 0

    # Check if repo_id is a local path
    is_local = data_config.repo_id.startswith("/") or data_config.repo_id.startswith(".")

    # åˆ›å»ºæ•°æ®é›†
    dataset = _data_loader.create_torch_dataset(data_config, action_horizon, model_config)

    # åˆ›å»ºåŒ…è£…å™¨ï¼ŒåªåŠ è½½ä»æŒ‡å®šæ ·æœ¬å¼€å§‹çš„æ•°æ®
    class SubsetDataset:
        def __init__(self, dataset, start_idx):
            self.dataset = dataset
            self.start_idx = start_idx
            self.original_length = len(dataset)
            self.new_length = self.original_length - start_idx

        def __len__(self):
            return max(0, self.new_length)

        def __getitem__(self, idx):
            # æ˜ å°„åˆ°åŸå§‹æ•°æ®é›†çš„ç´¢å¼•
            original_idx = self.start_idx + idx
            if original_idx >= self.original_length:
                raise IndexError(f"Index {original_idx} out of range for dataset of length {self.original_length}")
            return self.dataset[original_idx]

    # åº”ç”¨å­é›†
    if start_from_sample > 0:
        print(f"ğŸ” ç›´æ¥ä»ç¬¬ {start_from_sample} ä¸ªæ ·æœ¬å¼€å§‹æµ‹è¯•")
        print(f"ğŸ“Š è·³è¿‡å‰ {start_from_sample} ä¸ªæ ·æœ¬")
        print(f"ğŸ“ˆ å°†æµ‹è¯• {len(dataset) - start_from_sample} ä¸ªæ ·æœ¬")
        dataset = SubsetDataset(dataset, start_from_sample)

    dataset = _data_loader.TransformedDataset(
        dataset,
        [
            *data_config.repack_transforms.inputs,
            *data_config.data_transforms.inputs,
            RemoveStrings(),
        ],
    )

    if max_frames is not None and max_frames < len(dataset):
        num_batches = max_frames // batch_size
        shuffle = True
    else:
        num_batches = len(dataset) // batch_size
        shuffle = False

    data_loader = _data_loader.TorchDataLoader(
        dataset,
        local_batch_size=batch_size,
        num_workers=num_workers,
        shuffle=shuffle,
        num_batches=num_batches,
    )
    return data_loader, num_batches


def main(config_name: str, start_from_sample: int = 8000, max_frames: int | None = None):
    print("=" * 60)
    print(f"ğŸ§ª æµ‹è¯•ä¿®å¤æ•ˆæœ - ä»ç¬¬ {start_from_sample} ä¸ªæ ·æœ¬å¼€å§‹")
    print("=" * 60)

    config = _config.get_config(config_name)
    data_config = config.data.create(config.assets_dirs, config.model)

    if data_config.rlds_data_dir is not None:
        data_loader, num_batches = create_rlds_dataloader(
            data_config, config.model.action_horizon, config.batch_size, max_frames
        )
    else:
        data_loader, num_batches = create_torch_dataloader(
            data_config, config.model.action_horizon, config.batch_size, config.model, config.num_workers, max_frames, start_from_sample
        )

    # åˆå§‹åŒ–è¿è¡Œæ—¶ç»Ÿè®¡å™¨
    keys = ["state", "actions"]
    stats = {key: normalize.RunningStats() for key in keys}

    # è®¡æ•°å™¨
    processed_batches = 0
    skipped_batches = 0
    error_samples = 0

    print(f"ğŸ“‹ å‡†å¤‡å¤„ç† {num_batches} ä¸ªæ‰¹æ¬¡")
    print(f"âš¡ å¼€å§‹ä»æ ·æœ¬ {start_from_sample} å¤„ç†...")

    # éå†æ•°æ®é›†è®¡ç®—ç»Ÿè®¡é‡
    for batch in tqdm.tqdm(data_loader, total=num_batches, desc="Testing from sample 8000"):
        try:
            for key in keys:
                if key in batch:
                    stats[key].update(np.asarray(batch[key]))
            processed_batches += 1

            # æ¯å¤„ç†100ä¸ªæ‰¹æ¬¡æŠ¥å‘Šä¸€æ¬¡
            if processed_batches % 100 == 0:
                print(f"âœ… å·²å¤„ç† {processed_batches} ä¸ªæ‰¹æ¬¡")

        except Exception as e:
            skipped_batches += 1
            error_samples += 1
            print(f"âŒ ç¬¬ {processed_batches + skipped_batches} æ‰¹æ¬¡å‡ºé”™: {str(e)[:100]}...")
            continue

    print("\n" + "=" * 60)
    print("ğŸ‰ æµ‹è¯•å®Œæˆ!")
    print(f"âœ… æˆåŠŸå¤„ç†æ‰¹æ¬¡: {processed_batches}")
    print(f"âŒ è·³è¿‡æ‰¹æ¬¡: {skipped_batches}")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {(processed_batches / (processed_batches + skipped_batches)) * 100:.1f}%")

    if processed_batches > 0:
        print("\nğŸ“Š è®¡ç®—çš„ç»Ÿè®¡é‡:")
        for key in keys:
            stats_dict = stats[key].get_statistics()
            print(f"  {key}: mean={stats_dict['mean']:.4f}, std={stats_dict['std']:.4f}")

        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ ·æœ¬
        total_samples = processed_batches * config.batch_size
        print(f"\nğŸ“Š å¤„ç†çš„æ ·æœ¬æ€»æ•°: {total_samples}")

        if total_samples > 1000:
            print("âœ… æ ·æœ¬æ•°é‡è¶³å¤Ÿï¼Œè®¡ç®—ç»“æœå¯ä¿¡")
        else:
            print("âš ï¸  æ ·æœ¬æ•°é‡è¾ƒå°‘ï¼Œå»ºè®®æ£€æŸ¥æ˜¯å¦æœ‰é—®é¢˜")

    print("=" * 60)


if __name__ == "__main__":
    tyro.cli(main)
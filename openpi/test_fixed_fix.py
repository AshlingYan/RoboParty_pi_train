#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæµ‹è¯•è„šæœ¬ï¼ŒéªŒè¯ä¿®å¤æ•ˆæœ
"""

import numpy as np
import tqdm
import tyro

import openpi.models.model as _model
import openpi.training.config as _config
import openpi.training.data_loader as _data_loader
import openpi.transforms as transforms


class RemoveStrings(transforms.DataTransformFn):
    def __call__(self, x: dict) -> dict:
        return {k: v for k, v in x.items() if not np.issubdtype(np.asarray(v).dtype, np.str_)}


def main(config_name: str, start_from_sample: int = 8000, max_frames: int | None = None):
    print("=" * 60)
    print(f"ğŸ§ª éªŒè¯ä¿®å¤æ•ˆæœ - ä»ç¬¬ {start_from_sample} ä¸ªæ ·æœ¬å¼€å§‹")
    print("=" * 60)

    config = _config.get_config(config_name)
    data_config = config.data.create(config.assets_dirs, config.model)

    if data_config.rlds_data_dir is not None:
        print("âŒ ä¸æ”¯æŒRLDSæ•°æ®é›†")
        return

    # åˆ›å»ºæ•°æ®é›†
    print(f"ğŸ“‚ åŠ è½½æ•°æ®é›†: {data_config.repo_id}")
    dataset = _data_loader.create_torch_dataset(data_config, config.model.action_horizon, config.model)

    # åˆ›å»ºå­é›†ï¼Œåªå¤„ç†ä»æŒ‡å®šæ ·æœ¬å¼€å§‹çš„æ•°æ®
    class SubsetDataset:
        def __init__(self, dataset, start_idx):
            self.dataset = dataset
            self.start_idx = start_idx
            self.original_length = len(dataset)
            self.new_length = max(0, self.original_length - start_idx)

        def __len__(self):
            return self.new_length

        def __getitem__(self, idx):
            original_idx = self.start_idx + idx
            if original_idx >= self.original_length:
                raise IndexError(f"Index {original_idx} out of range")
            return self.dataset[original_idx]

    if start_from_sample > 0:
        print(f"ğŸ” ç›´æ¥ä»ç¬¬ {start_from_sample} ä¸ªæ ·æœ¬å¼€å§‹æµ‹è¯•")
        print(f"ğŸ“Š è·³è¿‡å‰ {start_from_sample} ä¸ªæ ·æœ¬")
        print(f"ğŸ“ˆ å°†æµ‹è¯• {len(dataset) - start_from_sample} ä¸ªæ ·æœ¬")
        dataset = SubsetDataset(dataset, start_from_sample)

    dataset = _data_loader.TransformedDataset(
        dataset,
        [
            *data_config.repack_transforms.inputs,
            *data_config.data_transforms.inputs,
            RemoveStrings(),
        ],
    )

    # è®¡ç®—æ‰¹æ¬¡æ•°é‡
    total_length = len(dataset)
    batch_size = config.batch_size
    num_batches = total_length // batch_size

    if max_frames is not None:
        num_batches = min(num_batches, max_frames // batch_size)

    print(f"ğŸ“‹ å‡†å¤‡å¤„ç† {num_batches} ä¸ªæ‰¹æ¬¡")
    print(f"âš¡ å¼€å§‹ä»æ ·æœ¬ {start_from_sample} å¤„ç†...")

    # ç®€å•ç»Ÿè®¡
    processed_samples = 0
    error_count = 0
    success_count = 0

    print("ğŸ” æµ‹è¯•æ•°æ®å¤„ç†æµç¨‹...")

    # æ‰‹åŠ¨éå†æ•°æ®é›†
    for i in tqdm.tqdm(range(num_batches), desc="Testing data loading"):
        try:
            batch_start = i * batch_size
            batch_end = min((i + 1) * batch_size, total_length)

            # æ¨¡æ‹Ÿè·å–ä¸€ä¸ªæ‰¹æ¬¡
            sample_indices = list(range(batch_start, batch_end))
            batch_data = []

            for idx in sample_indices:
                try:
                    data = dataset[idx]
                    batch_data.append(data)
                    success_count += 1
                except Exception as e:
                    error_count += 1
                    if "tolerance" in str(e):
                        continue  # è·³è¿‡å®¹å·®é”™è¯¯
                    elif "frame" in str(e).lower():
                        continue  # è·³è¿‡å¸§é”™è¯¯
                    else:
                        # è®°å½•å…¶ä»–é”™è¯¯
                        continue

            processed_samples += len(batch_data)
            if i % 50 == 0:
                print(f"âœ… å·²å¤„ç† {i+1}/{num_batches} ä¸ªæ‰¹æ¬¡")

        except Exception as e:
            error_count += 1
            continue

    print("\n" + "=" * 60)
    print("ğŸ‰ æµ‹è¯•å®Œæˆ!")
    print(f"âœ… æˆåŠŸå¤„ç†æ ·æœ¬: {success_count}")
    print(f"âŒ é”™è¯¯æ ·æœ¬: {error_count}")
    print(f"ğŸ“Š æˆåŠŸç‡: {(success_count / (success_count + error_count)) * 100:.1f}%")
    print(f"ğŸ“ˆ å¤„ç†çš„æ ·æœ¬æ€»æ•°: {processed_samples}")

    # æœ€ç»ˆåˆ¤æ–­
    if processed_samples > 100:
        print("âœ… ä¿®å¤æˆåŠŸï¼æœ‰è¶³å¤Ÿçš„æ ·æœ¬è¢«æˆåŠŸå¤„ç†")
        print("ğŸ¯ ç°åœ¨å¯ä»¥è¿è¡Œå®Œæ•´çš„ compute_norm_stats")
    else:
        print("âš ï¸  å¤„ç†çš„æ ·æœ¬æ•°é‡è¾ƒå°‘ï¼Œå¯èƒ½è¿˜éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´")

    print("=" * 60)


if __name__ == "__main__":
    tyro.cli(main)